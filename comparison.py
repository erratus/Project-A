import os
import json
import argparse
import re
import time
from chromadb import PersistentClient
import requests

# === Load Hugging Face Token ===


# === Paths ===
jd_db_path = "../chroma_db_jd"
resume_db_path = "../chroma_db_resume"
model="bigllama/mistralv01-7b:latest"

FIELD_ORDER = ["Skills", "Education", "Experience", "Job Role"]

# === Prompt Templates ===
system_prompt = """ You are a world-class HR, Talent Acquisition, and Generative AI Specialist with deep expertise in job-role alignment, semantic document comparison, and hiring decision automation.

You are tasked with comparing a candidate resume and a job description. Both are pre-parsed into structured fields: Skills, Education, Job Role, Experience, and Other Information. Your job is to assess the alignment **strictly based on meaning** — not exact keyword matches.

You must return a single valid JSON object in the structure described below.

### Instructions:

- Evaluate **semantic relevance**, not just keyword overlap. For example, treat "ML Engineer" and "Machine Learning Engineer" as identical.
- Use **real-world hiring logic**: If a resume **exceeds** the JD requirements (e.g., more skills, more education, deeper experience), the match_pct should be high — even **100%**.
- Avoid over-penalizing minor differences. Focus on **capability and fit**.
- NEVER hallucinate or infer information not explicitly present in either document.
- NEVER nest objects inside any field — your output must remain a **flat JSON**.
- Explanations must be **insightful, human-readable, and professional** — written as if speaking to a hiring manager.
- Escape any invalid characters like tabs/newlines using `\\t` and `\\n`.
- Do **not** include any commentary or text outside the JSON.

### Field Matching Logic:

1. **Skills**
   - Match based on technical equivalence.
   - If the resume includes **all** required skills or **more**, assign **100%**.
   - If semantically similar (e.g., "pandas" vs. "data manipulation in Python"), still assign high match_pct (80–95%).

2. **Education**
   - If the candidate’s education level is **equal or higher** than the JD, score high.
   - Degrees in relevant fields or from reputable institutions should be favored.

3. **Experience**
   - Match on role relevance, technologies used, domain familiarity, and years of experience.
   - Experience that directly meets or exceeds JD expectations should score high (90–100%).

4. **Job Role**
   - Normalize semantically equivalent titles (e.g., "ML Engineer" = "Machine Learning Engineer").
   - If the resume job role **exactly matches** or semantically aligns with **any title** in the job description (e.g., "Generative AI Engineer" in both), assign **100%**.
   - If the resume title is a **parent or superset role** of the JD (e.g., "Data Scientist" when JD asks for "ML Engineer"), assign a **high match percentage (90–95%)**.
   - If the resume title is a **subset** (e.g., "ML Engineer" when JD includes "Generative AI Engineer"), still assign **high score (90–95%)**, if contextually relevant.
   - Recognize hierarchical and domain relationships:
     - Example: "Data Scientist" includes "ML Engineer" and "Generative AI Engineer" as subdomains.
     - "Machine Learning Engineer" is semantically equal to "ML Engineer".
   - Penalize only when there is a **significant deviation** in domain, seniority, or function (e.g., "Project Manager" vs. "ML Engineer").
   - Prioritize domain alignment, relevance to AI/ML/Generative AI, and explain clearly why a role is penalized or rewarded.


5. **OverallMatchPercentage**
   - Must be a weighted score calculated **heavily** from Skills, Experience, Education, and Job Role.
   - **Other Information** may provide a minor bonus/penalty (±5%).

6. **AI_Generated_Estimate_Percentage**
   - Estimate how likely the resume was generated by AI, based on repetition, unnatural tone, excessive perfection, or generic phrasing.

### Output Format (strict):

{
  "{resume_filename}": {
    "Skills": {
      "match_pct": float,
      "resume_value": string,
      "job_description_value": string,
      "explanation": string
    },
    "Education": {
      "match_pct": float,
      "resume_value": string,
      "job_description_value": string,
      "explanation": string
    },
    "Job Role": {
      "match_pct": float,
      "resume_value": string,
      "job_description_value": string,
      "explanation": string
    },
    "Experience": {
      "match_pct": float,
      "resume_value": string,
      "job_description_value": string,
      "explanation": string
    },
    "OverallMatchPercentage": float,
    "why_overall_match_is_this": string,
    "AI_Generated_Estimate_Percentage": float
  }
}

Return ONLY the JSON object. No extra comments or explanation."""

user_prompt_template = """Compare the following resume and job description using their parsed field data.

Each field below is populated from the database. Compare them **semantically and intelligently** using the structure below.

Use the following strict JSON format:

{{
  "{resume_filename}": {{
    "Skills": {{
      "match_pct": float,
      "resume_value": string,
      "job_description_value": string,
      "explanation": string
    }},
    "Education": {{
      "match_pct": float,
      "resume_value": string,
      "job_description_value": string,
      "explanation": string
    }},
    "Job Role": {{
      "match_pct": float,
      "resume_value": string,
      "job_description_value": string,
      "explanation": string
    }},
    "Experience": {{
      "match_pct": float,
      "resume_value": string,
      "job_description_value": string,
      "explanation": string
    }},
    "OverallMatchPercentage": float,
    "why_overall_match_is_this": string,
    "AI_Generated_Estimate_Percentage": float
  }}
}}

Return only the JSON object, and ensure:
- match_pct values reflect real semantic similarity (not keyword count).
- Explanations are professional, specific, and insightful.
- No nested JSON objects inside any value fields.
- No semicolons (;) in values — use periods or commas.
- No hallucinated info or missing keys.
"""

def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--resume_filename', type=str, default="resume.docx")
    return parser.parse_args()

def get_all_collections(client):
    try:
        return [c.name for c in client.list_collections()]
    except Exception as e:
        print(f"[ERROR] Failed to list collections: {e}")
        return []

def get_collection_docs(client, collection_name):
    try:
        collection = client.get_collection(collection_name)
        results = collection.get(include=["documents"])
        docs = results.get("documents", [])
        if docs and isinstance(docs[0], list):
            docs = [d for sublist in docs for d in sublist]
        return docs
    except Exception as e:
        print(f"[ERROR] Failed to load collection '{collection_name}': {e}")
        return []

def build_field_texts(field_names, docs):
    lines = []
    for name, doc in zip(field_names, docs):
        doc_clean = re.sub(rf"^{re.escape(name)}:\s*", "", doc, flags=re.IGNORECASE)
        lines.append(f"{name}: {doc_clean}")
    return "\n".join(lines)

def clean_llm_json(raw_response):
    raw = raw_response.strip()
    raw = re.sub(r"^```(?:json)?|```$", "", raw, flags=re.MULTILINE).strip()
    brace_stack = []
    start_idx = None
    for i, char in enumerate(raw):
        if char == '{':
            if start_idx is None:
                start_idx = i
            brace_stack.append('{')
        elif char == '}':
            if brace_stack:
                brace_stack.pop()
                if not brace_stack:
                    return raw[start_idx:i+1].strip()
    return raw

def normalize_llm_response(data):
    for field in ["Skills", "Education", "Job Role", "Experience"]:
        if field in data:
            for key in ["resume_value", "job_description_value"]:
                value = data[field].get(key)
                if isinstance(value, list):
                    data[field][key] = ", ".join(map(str, value))
    return data

def query_llm(system_prompt, user_prompt, retries=2, model="bigllama/mistralv01-7b:latest", temperature=0.0, seed=42):
    url = "http://localhost:11434/api/generate"
    headers = {"Content-Type": "application/json"}
    
    # Concatenate prompts for non-chat model interface
    prompt = f"<|system|>\n{system_prompt.strip()}\n<|user|>\n{user_prompt.strip()}\n"

    payload = {
        "model": model,
        "prompt": prompt,
        "temperature": temperature,
        "seed": seed,
        "stream": False
    }

    for attempt in range(retries):
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            return response.json()["response"].strip()
        except Exception as e:
            print(f"[ERROR] Ollama call failed (attempt {attempt + 1}): {e}")
            time.sleep(1)
    return ""



def main():
    start_time = time.time()
    args = get_args()

    jd_client = PersistentClient(path=jd_db_path)
    resume_client = PersistentClient(path=resume_db_path)

    jd_collections = get_all_collections(jd_client)
    resume_collections = get_all_collections(resume_client)

    all_results = []

    for jd_collection in jd_collections:
        jd_docs = get_collection_docs(jd_client, jd_collection)
        if len(jd_docs) < 5:
            continue

        jd_text = build_field_texts(FIELD_ORDER, jd_docs[:4])
        jd_other_info = jd_docs[4]

        for resume_collection in resume_collections:
            resume_docs = get_collection_docs(resume_client, resume_collection)
            if len(resume_docs) < 5:
                continue

            resume_text = build_field_texts(FIELD_ORDER, resume_docs[:4])
            resume_other_info = resume_docs[4]

            comparison_name = f"{resume_collection}_vs_{jd_collection}"

            user_prompt = user_prompt_template.format(resume_filename=comparison_name)
            user_prompt += f"\n\nJob Description Other Information:\n{jd_other_info}"
            user_prompt += f"\nResume Other Information:\n{resume_other_info}"
            user_prompt += f"\n\nJob Description:\n{jd_text}\n\nResume:\n{resume_text}"

            raw = query_llm(system_prompt, user_prompt)
            if not raw:
                continue

            try:
                cleaned = clean_llm_json(raw)
                parsed = json.loads(cleaned)
                parsed = {k: normalize_llm_response(v) for k, v in parsed.items()}
                all_results.append(parsed)
            except Exception as e:
                print(f"[ERROR] Failed parsing response for {comparison_name}: {e}")
                with open(f"debug_{comparison_name}.txt", "w", encoding="utf-8") as f:
                    f.write(raw)

    with open("comparison_results_all.json", "w", encoding="utf-8") as f:
        json.dump(all_results, f, indent=4)

    print(json.dumps(all_results, indent=2))

    print("\n=========== Summary of Matches ===========\n")
    summary = {}
    for result in all_results:
        for key, data in result.items():
            resume, _, jd = key.partition("_vs_")
            summary.setdefault(jd, []).append((resume, data.get("OverallMatchPercentage", 0)))

    for jd, matches in summary.items():
        print(f"Job: {jd}")
        for resume, score in matches:
            print(f"  - {resume}: {score:.1f}%")
        print()

    print(f"\n[INFO] Total time: {time.time() - start_time:.2f} sec")

if __name__ == "__main__":
    main()
